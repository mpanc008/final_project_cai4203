{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpanc008/final_project_cai4203/blob/Tuntu/fin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71ec6b5b408ee99",
      "metadata": {
        "id": "d71ec6b5b408ee99"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-10T16:58:58.965926Z",
          "start_time": "2025-07-10T16:58:58.926940Z"
        },
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "953f68d0",
      "metadata": {
        "id": "953f68d0",
        "outputId": "44824558-8357-432e-8ebc-a23e02ccb7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wiki.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65G/1.65G [00:06<00:00, 265MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping wiki.zip...\n",
            "Downloading inpainting.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.01G/1.01G [00:03<00:00, 322MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping inpainting.zip...\n",
            "Downloading insight.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.01G/1.01G [00:03<00:00, 283MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping insight.zip...\n",
            "Downloading text2img.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.06G/1.06G [00:03<00:00, 325MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping text2img.zip...\n",
            "Real images: 30000\n",
            "Fake images: 30000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "RUNNING_ON_COLAB = True\n",
        "\n",
        "def download_and_unzip(url, extract_to):\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        total = int(response.headers.get(\"content-length\", 0))\n",
        "        with tqdm.wrapattr(f, \"write\", total=total) as out_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                out_file.write(chunk)\n",
        "    print(f\"Unzipping {filename}...\")\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    os.remove(filename)\n",
        "\n",
        "def flatten_folder(source_dir, target_dir):\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    for root, dirs, files in os.walk(source_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                src = os.path.join(root, file)\n",
        "                dst = os.path.join(target_dir, file)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "if RUNNING_ON_COLAB:\n",
        "    os.makedirs(\"data/real\", exist_ok=True)\n",
        "    os.makedirs(\"data/fake\", exist_ok=True)\n",
        "\n",
        "    urls = {\n",
        "        \"wiki\": \"https://huggingface.co/datasets/OpenRL/DeepFakeFace/resolve/main/wiki.zip\",\n",
        "        \"inpainting\": \"https://huggingface.co/datasets/OpenRL/DeepFakeFace/resolve/main/inpainting.zip\",\n",
        "        \"insight\": \"https://huggingface.co/datasets/OpenRL/DeepFakeFace/resolve/main/insight.zip\",\n",
        "        \"text2img\": \"https://huggingface.co/datasets/OpenRL/DeepFakeFace/resolve/main/text2img.zip\"\n",
        "    }\n",
        "\n",
        "    for name, url in urls.items():\n",
        "        extract_dir = f\"temp/{name}\"\n",
        "        download_and_unzip(url, extract_dir)\n",
        "        target = \"data/real\" if name == \"wiki\" else \"data/fake\"\n",
        "        flatten_folder(extract_dir, target)\n",
        "\n",
        "else:\n",
        "    # Your local logic here\n",
        "    flatten_folder(\"C:/data/wiki\", \"data/real\")\n",
        "    flatten_folder(\"C:/data/inpainting\", \"data/fake\")\n",
        "    flatten_folder(\"C:/data/insight\", \"data/fake\")\n",
        "    flatten_folder(\"C:/data/text2img\", \"data/fake\")\n",
        "\n",
        "print(\"Real images:\", len(os.listdir(\"data/real\")))\n",
        "print(\"Fake images:\", len(os.listdir(\"data/fake\")))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ee301f12cb2b15",
      "metadata": {
        "id": "50ee301f12cb2b15"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c7791aedfe16eb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-10T17:50:32.516153Z",
          "start_time": "2025-07-10T17:49:45.170615Z"
        },
        "id": "73c7791aedfe16eb",
        "outputId": "6f2d4e6d-1626-4ff6-91bc-3b99f3f219df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60000 files belonging to 2 classes.\n",
            "Using 48000 files for training.\n",
            "Found 60000 files belonging to 2 classes.\n",
            "Using 12000 files for validation.\n",
            "Class names: ['fake', 'real']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# --------- CONFIG ---------\n",
        "image_size = (224, 224)\n",
        "batch_size = 200\n",
        "data_dir = \"data\"  # Make sure 'data/real/' and 'data/fake/' exist\n",
        "\n",
        "# --------- LOAD RAW DATASETS ---------\n",
        "# Load before any .map() so we can access class_names\n",
        "raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "raw_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "# --------- CLASS NAMES ---------\n",
        "class_names = raw_train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "# --------- NORMALIZATION & PIPELINE OPTIMIZATION ---------\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = (\n",
        "    raw_train_ds\n",
        "    .map(lambda x, y: (normalization_layer(x), y))\n",
        "    .cache()\n",
        "    .shuffle(1000)\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    raw_val_ds\n",
        "    .map(lambda x, y: (normalization_layer(x), y))\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca29b7e",
      "metadata": {
        "id": "dca29b7e",
        "outputId": "b354929c-81f0-4e84-d7cc-17b0706f3986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU not found\n",
            "Running on CPU/GPU\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 8s/step - accuracy: 0.5038 - loss: 0.6980\n",
            "\n",
            "✅ Evaluation Results:\n",
            "Test loss: 0.6963\n",
            "Test accuracy: 0.5090\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2043945753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#Accuracy Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "# -------- MODEL DEFINITION ---------\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.Dense(650, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.Dense(304, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.Dense(161, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.Dense(80, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# --------- TRAINING ---------\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"TPU found:\", resolver.cluster_spec())\n",
        "except ValueError:\n",
        "    print(\"TPU not found\")\n",
        "\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"Running on TPU\")\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()  # default CPU/GPU strategy\n",
        "    print(\"Running on CPU/GPU\")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.20),\n",
        "        tf.keras.layers.Dense(650, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.20),\n",
        "        tf.keras.layers.Dense(304, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.20),\n",
        "        tf.keras.layers.Dense(161, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.20),\n",
        "        tf.keras.layers.Dense(80, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.20),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "#batch_size?\n",
        "\n",
        "# --------- EVALUATE ON VALIDATION SET ---------\n",
        "loss, acc = model.evaluate(val_ds, verbose=1)\n",
        "\n",
        "print(f\"\\n✅ Evaluation Results:\")\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "print(f\"Test accuracy: {acc:.4f}\")\n",
        "\n",
        "#---Plot\n",
        "\n",
        "\n",
        "#Accuracy Plot\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Loss Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}